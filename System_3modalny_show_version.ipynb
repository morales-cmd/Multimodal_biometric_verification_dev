{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "System_3modalny_show_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morales-cmd/Multimodal_biometric_verification_dev/blob/main/System_3modalny_show_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfQ_mmZKmTOD",
        "outputId": "04b41dd5-2cbc-4393-a0d6-134fd0dbe66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Drive to Colab notebook drive mounting.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckwt_ucewdgU",
        "outputId": "9b36ff67-8d1e-49da-8b93-13b766d8edac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries.\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "import cv2\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras import datasets, layers, models,optimizers, metrics\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "import scipy\n",
        "import numpy as np\n",
        "from numpy.polynomial.polynomial import Polynomial\n",
        "import cv2\n",
        "import librosa, librosa.display\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from timeit import default_timer as timer\n",
        "from random import random\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "1W-P42RxwF_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Signature feature extraction"
      ],
      "metadata": {
        "id": "8PyP79frAsOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# euclidean distnace between 2 points (input 2 points)\n",
        "def distance(curpoint,prevpoint):\n",
        "  return np.sqrt((curpoint[0]-prevpoint[0])**2+(curpoint[1]-prevpoint[1])**2)\n",
        "\n",
        "# return acc stats (min,max, mean, zerocrossings) stat (input non-stationary points with vel and acc)\n",
        "def acceleration_stats(points):\n",
        "  arr=np.array(points)\n",
        "  arr=np.transpose(arr)\n",
        "  zero_crossings = librosa.zero_crossings(arr[6], pad=False)\n",
        "  return(min(arr[6]),max(arr[6]),np.mean(arr[6]),sum(zero_crossings))\n",
        "\n",
        "# return vector with acc/dur stat (input non-stationary points with vel and acc)\n",
        "def acceleration_by_duration(points):\n",
        "  sum=0\n",
        "  for i in range(1,len(points)):\n",
        "    if(points[i][6]>0):\n",
        "      sum+= points[i][4]-points[i-1][4]\n",
        "  dur=points[len(points)-1][4]-points[0][4]\n",
        "  return sum/dur\n",
        "\n",
        "# return vector with all strokes durations (input non-stationary points)\n",
        "def calc_vel_acc(points):\n",
        "  points[0].append(0)  #0 velocity at 1st point\n",
        "  points[0].append(0)  #0 acceleration at 1st point\n",
        "\n",
        "  for i in range(1,len(points)):\n",
        "    dist=distance(points[i],points[i-1])\n",
        "    velocity= dist/(points[i][4]-points[i-1][4])\n",
        "    points[i].append(velocity)\n",
        "    acceleration=(points[i][5]-points[i-1][5])/(points[i][4]-points[i-1][4])\n",
        "    points[i].append(acceleration)\n",
        "\n",
        "# return vector with all strokes durations (input all points)\n",
        "def stroke_dur(points):\n",
        "  strokes_durs=[]\n",
        "  start=points[0][4]\n",
        "  for i in range(1,len(points)):\n",
        "    if(points[i][3]>0 and points[i-1][3]==0):\n",
        "      start=points[i][4]\n",
        "    if(points[i][3]==0 and points[i-1][3]>0):\n",
        "      strokes_durs.append(points[i][4]-start)\n",
        "  return strokes_durs\n",
        "def angle(v1,v2):\n",
        "  return np.arccos(np.dot(np.array(v1),np.array(v2))/(np.linalg.norm(v1)*np.linalg.norm(v2)))\n",
        "\n",
        "\n",
        "def calc_dir_change_vel(points):\n",
        "  points[0].append(0)  #0 velocity at 1st point\n",
        "  points[1].append(0)  #0 acceleration at 1st point\n",
        "\n",
        "  for i in range(2,len(points)):\n",
        "    v1=[points[i][0]-points[i-1][0],points[i][1]-points[i-1][1]]\n",
        "    v2=[points[i-1][0]-points[i-2][0],points[i-1][1]-points[i-2][1]]\n",
        "\n",
        "    ang=angle(v1,v2);\n",
        "    velocity= ang/(points[i][4]-points[i-1][4])\n",
        "    # print(velocity)\n",
        "    points[i].append(velocity)\n",
        "    # acceleration=(points[i][5]-points[i-1][5])/(points[i][4]-points[i-1][4])\n",
        "    # points[i].append(acceleration)\n",
        "\n",
        "\n",
        "# return mean and std of dir change acceleration\n",
        "def dir_vel_stats(points):\n",
        "  arr=np.array(points)\n",
        "  arr=np.transpose(arr)\n",
        "  # print(np.nanmean(arr[7]))\n",
        "  return arr[7]\n",
        "\n",
        "# return vector with all strokes lengths (input all points)\n",
        "def stroke_len(points):\n",
        "  strokes_lens=[]\n",
        "  sum=0\n",
        "  if points[0][3]>0:\n",
        "    start=1\n",
        "  else:\n",
        "    start=0\n",
        "  for i in range(1,len(points)):\n",
        "    if(points[i][3]>0 and points[i-1][3]==0):\n",
        "      start=1\n",
        "    if(points[i][3]==0 and points[i-1][3]>0):\n",
        "      start=0\n",
        "      strokes_lens.append(sum)\n",
        "      sum=0\n",
        "    if(start==1):\n",
        "      sum+=distance(points[i],points[i-1])\n",
        "  return strokes_lens\n",
        "\n",
        "def average_stroke_press(points):\n",
        "  strokes_press=[]\n",
        "  it = 0\n",
        "  sum = 0\n",
        "  if points[0][3]>0:\n",
        "    start=1\n",
        "  else:\n",
        "    start=0\n",
        "  for i in range(1,len(points)):\n",
        "    if(points[i][3]>0 and points[i-1][3]==0):\n",
        "      it=0\n",
        "      sum=0\n",
        "      start=1\n",
        "    if(points[i][3]==0 and points[i-1][3]>0):\n",
        "      start=0\n",
        "      strokes_press.append(sum/it)\n",
        "    if(start==1):\n",
        "      sum+=points[i][3]\n",
        "      it+=1\n",
        "  return strokes_press\n",
        "\n",
        "#visualization of signature\n",
        "def vizualization(points):\n",
        "  im=np.zeros((200,200))\n",
        "  for p in points:\n",
        "    im[200-int(p[1]),int(p[0])]=255\n",
        "  cv2.imshow(im)\n",
        "\n",
        "# remove points with no x and y changes or no time changes\n",
        "def remove_stationary_points(points):\n",
        "  new_points=[]\n",
        "  for i in range(1,len(points)):\n",
        "    if((points[i][0]!=points[i-1][0] or points[i][1]!=points[i-1][1])and (points[i][4]!=points[i-1][4])):\n",
        "      new_points.append(points[i])\n",
        "  return new_points\n",
        "def remove_noisy_points(points):\n",
        "  new_points=[]\n",
        "  for i in range(1,len(points)-1):\n",
        "    if(~(points[i][3]==0 and points[i-1][3]>0 and points[i+1][3]>0)):\n",
        "      new_points.append(points[i])\n",
        "  return new_points\n",
        "\n",
        "class Signature_features:\n",
        "  def __init__(self, acc_by_dur, m_str_dur,std_str_dur,m_str_len,std_str_len, acc_stats,m_press,std_press):\n",
        "    self.acc_by_dur = acc_by_dur\n",
        "    self.m_str_dur = m_str_dur\n",
        "    self.std_str_dur=std_str_dur\n",
        "    self.m_str_len = m_str_len\n",
        "    self.std_str_len=std_str_len\n",
        "    self.acc_stats = acc_stats\n",
        "    self.m_press= m_press\n",
        "    self.std_press=std_press\n",
        "\n",
        "# 0 acc_by_dur\n",
        "# 1 m_str_dur\n",
        "# 2 std_str_dur\n",
        "# 3 m_str_len\n",
        "# 4 std_str_len\n",
        "# 5 m_str_len\n",
        "# 6 min_acc\n",
        "# 7 max_acc\n",
        "# 8 mean_acc\n",
        "# 9 zrc_acc\n",
        "# 10 m_press\n",
        "# 11 std_press\n",
        "def get_stats(lines):\n",
        "  points=[]\n",
        "  for l in lines:\n",
        "    a=l.replace(\"\\n\",\"\")\n",
        "    a=a.split(\", \")\n",
        "    a=[float(numeric_string) for numeric_string in a]\n",
        "    points.append(a)\n",
        "  points_reduced=remove_stationary_points(points)\n",
        "  points_filtered=remove_noisy_points(points)\n",
        "  calc_vel_acc(points_reduced)\n",
        "  calc_dir_change_vel(points_reduced)\n",
        "  return [acceleration_by_duration(points_reduced), np.mean(stroke_dur(points_filtered)),np.std(stroke_dur(points_filtered)), np.mean(stroke_len(points_filtered)), np.std(stroke_len(points_filtered)), acceleration_stats(points_reduced)[0],acceleration_stats(points_reduced)[1],acceleration_stats(points_reduced)[2],acceleration_stats(points_reduced)[3],np.mean(average_stroke_press(points_filtered)), np.std(average_stroke_press(points_filtered)),np.nanmean(dir_vel_stats(points_reduced)),np.nanstd(dir_vel_stats(points_reduced))]\n",
        "\n",
        "def read_signature(path):\n",
        "  with open(path) as f:\n",
        "    lines = f.readlines()\n",
        "  return lines"
      ],
      "metadata": {
        "id": "ye9sl7a62TmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sets preparation modules"
      ],
      "metadata": {
        "id": "F3tjd1cpA0nU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting sets to train and test"
      ],
      "metadata": {
        "id": "cfnuPKdu3tdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_mono_set(array, labels,seed):\n",
        "    traincX = []\n",
        "    trainY = []\n",
        "    testcX = []\n",
        "    testY = []\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "    for train_index, test_index in sss.split(array, labels):\n",
        "        for ind in train_index:\n",
        "            traincX.append(array[ind])\n",
        "            trainY.append(labels[ind])\n",
        "        for ind in test_index:\n",
        "            testcX.append(array[ind])\n",
        "            testY.append(labels[ind])\n",
        "    return traincX,trainY,testcX,testY"
      ],
      "metadata": {
        "id": "molf4iNddolA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging of biometric features with cartesian product (with respect to owner)"
      ],
      "metadata": {
        "id": "s6LoCK0h3_Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_sets_cartesian(img_matrix,signature_matrix,wrist_matrix,image_labels,signature_labels,wrist_labels):\n",
        "  merged_array=[]\n",
        "  new_labels=[]\n",
        "  for count ,sig in enumerate(signature_matrix):\n",
        "    for count2, wrist in enumerate(wrist_matrix):\n",
        "      if(wrist_labels[count2] == signature_labels[count]):\n",
        "        merged_array.append([signature_matrix[count], wrist_matrix[count2]])\n",
        "        wrist_labels=np.delete(wrist_labels, count2, 0)\n",
        "        wrist_matrix=np.delete(wrist_matrix, count2, 0)\n",
        "        new_labels.append(signature_labels[count])  \n",
        "        break\n",
        "  # print(img_matrix[0])\n",
        "  merged_array2=[]\n",
        "  new_labels2=[]\n",
        "  for count, mer in enumerate(merged_array):\n",
        "    for count2, img in enumerate(img_matrix):\n",
        "      if(image_labels[count2] == new_labels[count]):\n",
        "        merged_array2.append([img_matrix[count2], merged_array[count][0], merged_array[count][1]])\n",
        "        new_labels2.append(image_labels[count2])\n",
        "  # print(np.array(merged_array).shape)\n",
        "  # print(merged_array[0])\n",
        "  return merged_array2,new_labels2"
      ],
      "metadata": {
        "id": "rPTjKRHGKmsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple 1 - 1 merging of biometric features "
      ],
      "metadata": {
        "id": "EFv-22OB4pwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_sets_simple(img_matrix,signature_matrix,wrist_matrix,image_labels,signature_labels,wrist_labels):\n",
        "  merged_array=[]\n",
        "  new_labels=[]\n",
        "  for count ,sig in enumerate(signature_matrix):\n",
        "    for count2, img in enumerate(img_matrix):\n",
        "      if(image_labels[count2]==signature_labels[count]):\n",
        "\n",
        "        merged_array.append([img_matrix[count2], signature_matrix[count]])\n",
        "        image_labels=np.delete(image_labels, count2, 0)\n",
        "        img_matrix=np.delete(img_matrix, count2, 0)\n",
        "        new_labels.append(signature_labels[count])\n",
        "        break\n",
        "  for count ,mer in enumerate(merged_array):\n",
        "    for count2, wrist in enumerate(wrist_matrix):\n",
        "      if(wrist_labels[count2]==new_labels[count]):\n",
        "\n",
        "        merged_array[count].append(wrist_matrix[count2])\n",
        "        wrist_labels=np.delete(wrist_labels, count2, 0)\n",
        "        wrist_matrix=np.delete(wrist_matrix, count2, 0)\n",
        "        break\n",
        "  return merged_array,new_labels"
      ],
      "metadata": {
        "id": "nlnTfo5LLDLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sets_back(merged_matrix):\n",
        "    img_matrix=[]\n",
        "    sig_matrix=[]\n",
        "    wrist_matrix=[]\n",
        "    for  it in merged_matrix:\n",
        "        img_matrix.append(it[0])\n",
        "        sig_matrix.append(it[1])\n",
        "        wrist_matrix.append(it[2])\n",
        "    return img_matrix,sig_matrix, wrist_matrix"
      ],
      "metadata": {
        "id": "woIFK1Yt4g-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wrist feature extraction"
      ],
      "metadata": {
        "id": "kJzGq2wdA5ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_wrist_csv2(filename):\n",
        "  dataframe = pd.read_csv(filename, skiprows=3)\n",
        "  features=[]\n",
        "\n",
        "  dataframe = dataframe.sort_values(by='HostTimestamp (ms)')\n",
        "  dataframediff = dataframe[['X (dps)','Y (dps)','Z (dps)','HostTimestamp (ms)']].diff(periods=1)\n",
        "  dataframediff=dataframediff[dataframediff['HostTimestamp (ms)']>15]\n",
        "  dataframediff=dataframediff.head(80)\n",
        "  fs=1/np.mean(dataframediff['HostTimestamp (ms)'])*1000\n",
        "  \n",
        "  rms=librosa.feature.rms(y=np.array(dataframediff['X (dps)']))\n",
        "  features += [np.mean(rms)]\n",
        "  rms=librosa.feature.rms(y=np.array(dataframediff['Y (dps)']))\n",
        "  features += [np.mean(rms)]\n",
        "  # print(np.std(rms))\n",
        "  rms=librosa.feature.rms(y=np.array(dataframediff['Z (dps)']))\n",
        "  features += [np.mean(rms)]\n",
        "\n",
        "  zrc=librosa.feature.zero_crossing_rate(y=np.array(dataframediff['X (dps)']))\n",
        "  features += [np.mean(zrc)]\n",
        "  zrc=librosa.feature.zero_crossing_rate(y=np.array(dataframediff['Y (dps)']))\n",
        "  features += [np.mean(zrc)]\n",
        "  zrc=librosa.feature.zero_crossing_rate(y=np.array(dataframediff['Z (dps)']))\n",
        "  features += [np.mean(zrc)]\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "RPktgNyS5gc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision module\n"
      ],
      "metadata": {
        "id": "LM-JVrZwA-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision module of biometric system"
      ],
      "metadata": {
        "id": "b2aj5E8s5NuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_module(class_model,X, Y):\n",
        "  X=np.array(X)\n",
        "  Y=np.array(Y)\n",
        "  dim=X.ndim\n",
        "  if (dim==1):\n",
        "    X = np.array([X])\n",
        "    Y = np.array([Y])\n",
        "  res = np.array(class_model.predict_proba(X))\n",
        "  res_matrix = np.array([np.argmax(res,axis=1),np.max(res,axis=1)])\n",
        "  max_cond = np.array(Y) == res_matrix[0]\n",
        "  thresh_cond = decision_threshold < res_matrix[1]\n",
        "  test_res = thresh_cond * max_cond\n",
        "  # print(Y)\n",
        "  # print(res_matrix)\n",
        "  if(dim==1):\n",
        "    return test_res[0]\n",
        "  else:\n",
        "    return test_res"
      ],
      "metadata": {
        "id": "VrVQSelISAiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAR, FRR test\n"
      ],
      "metadata": {
        "id": "7ulYbJXABFUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FRR_test(class_model, testX, testY):\n",
        "  res = decision_module(class_model, testX, testY)\n",
        "  stat = np.sum(res)/res.shape[0]\n",
        "  stat = 1-stat\n",
        "  return stat"
      ],
      "metadata": {
        "id": "g5coHdx-naTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FAR_test(class_model, testX, testY):\n",
        "  # add=np.random.random_integers(1, np.max(np.array(testY)), size = np.array(testY).shape[0])\n",
        "  # testY = (testY+ add) % (np.max(np.array(testY))+1)\n",
        "  testX_f=[]\n",
        "  testY_f=[]\n",
        "  for i in range(len(testY)):\n",
        "    for j in range(np.max(np.array(testY))):\n",
        "      testX_f.append(testX[i])\n",
        "      testY_f.append((testY[i]+j+1) % (np.max(np.array(testY))+1))\n",
        "  # print(testY_f)\n",
        "  # print(testY)\n",
        "  testX_f=np.array(testX_f)\n",
        "  testY_f=np.array(testY_f)\n",
        "  res = decision_module(class_model, testX_f, testY_f)\n",
        "  stat = np.sum(res)/res.shape[0]\n",
        "  return stat"
      ],
      "metadata": {
        "id": "Xsb2tKZ6rASo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First model"
      ],
      "metadata": {
        "id": "N0E75OuN5i3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model parameters (PCA components, decision odule threshold)"
      ],
      "metadata": {
        "id": "TuH23ZNZ9n2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=26\n",
        "decision_threshold=0.3\n",
        "comp_img=20\n",
        "comp_sig=8\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "Q5LKBxJmxcgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data + feature extraction"
      ],
      "metadata": {
        "id": "T-84ZUjG97_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root=\"/content/drive/MyDrive/sem3_biom_bazdana\"\n",
        "labels = []\n",
        "image_matrix = []\n",
        "image_labels = []\n",
        "wrist_matrix=[]\n",
        "signature_features_matrix=[]\n",
        "signature_labels=[]\n",
        "wrist_labels=[]\n",
        "for user in os.listdir(f'{root}'):\n",
        "    labels.append(user)\n",
        "    for filename in os.listdir(f'{root}/{user}/faces'):\n",
        "\n",
        "        img = cv2.imread(f'{root}/{user}/faces/{filename}')\n",
        "        img = cv2.resize(img, (100, 100))\n",
        "        image_matrix.append(np.array(img))\n",
        "        image_labels.append(labels.index(user))\n",
        "    for filename in os.listdir(f'{root}/{user}/signatures'):\n",
        "        signature=read_signature(f'{root}/{user}/signatures/{filename}')\n",
        "        stats = get_stats(signature)\n",
        "        if stats[1]>0 and stats[0]<1:\n",
        "            signature_features_matrix.append(stats)\n",
        "            signature_labels.append(labels.index(user))\n",
        "    for filename in os.listdir(f'{root}/{user}/wrist_gyroscope'):\n",
        "        wrist_matrix.append(read_wrist_csv2(f'{root}/{user}/wrist_gyroscope/{filename}'))\n",
        "        wrist_labels.append(labels.index(user))\n",
        "IMG_SIZE = (100, 100)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "inputs = tf.keras.Input(shape=(100,100, 3))\n",
        "x = preprocess_input(inputs)\n",
        "x = base_model(x, training=False)\n",
        "outputs = global_average_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "image_matrix=model.predict(np.array(image_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCuC4YHUyEQh",
        "outputId": "c51312ff-8a9b-4b0a-ed47-9b348dcc5af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_sig_X, train_sig_Y, test_sig_X, test_sig_Y = split_mono_set(signature_features_matrix, signature_labels,seed)\n",
        "train_img_X, train_img_Y, test_img_X, test_img_Y = split_mono_set(image_matrix, image_labels,seed)\n",
        "train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y = split_mono_set(wrist_matrix, wrist_labels,seed)\n",
        "\n",
        "print(train_img_X[0].shape)\n",
        "\n",
        "# train_img_X=model.predict(np.array(train_img_X))\n",
        "# train_img_X=np.array(train_img_X)\n",
        "pca = PCA(svd_solver=\"randomized\",n_components=comp_img, whiten=True)\n",
        "train_img_X = pca.fit_transform(np.array(train_img_X))\n",
        "\n",
        "\n",
        "scaler_sig = StandardScaler()\n",
        "train_sig_X=scaler_sig.fit_transform(train_sig_X)\n",
        "pca_sig = PCA(svd_solver=\"randomized\", n_components=comp_sig, whiten=True)\n",
        "train_sig_X=pca_sig.fit_transform(train_sig_X)\n",
        "# train_sig_X=np.fliplr(train_sig_X)\n",
        "\n",
        "scaler_wrist= StandardScaler()\n",
        "train_wrist_X=scaler_wrist.fit_transform(train_wrist_X)\n",
        "pca_wrist = PCA(svd_solver=\"randomized\", n_components=comp_wrist, whiten=True)\n",
        "train_wrist_X=pca_wrist.fit_transform(train_wrist_X)\n",
        "# train_wrist_X=np.fliplr(train_wrist_X)\n",
        "\n",
        "test_img_X=pca.transform(np.array(test_img_X))\n",
        "\n",
        "test_sig_X = scaler_sig.transform(np.array(test_sig_X))\n",
        "test_sig_X = pca_sig.transform(test_sig_X)\n",
        "# test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "test_wrist_X = scaler_wrist.transform(np.array(test_wrist_X))\n",
        "test_wrist_X = pca_wrist.transform(test_wrist_X)\n",
        "# test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "merged_trainX, merged_trainY = merge_sets_cartesian(train_img_X, train_sig_X, train_wrist_X, train_img_Y,\n",
        "                                                     train_sig_Y,train_wrist_Y)\n",
        "train_img_X, train_sig_X, train_wrist_X = split_sets_back(merged_trainX)\n",
        "\n",
        "merged_testX, merged_testY = merge_sets_simple(test_img_X, test_sig_X, test_wrist_X, test_img_Y,\n",
        "                                                     test_sig_Y,test_wrist_Y)\n",
        "test_img_X, test_sig_X, test_wrist_X = split_sets_back(merged_testX)\n",
        "\n",
        "\n",
        "trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X)),axis=1)\n",
        "trainY=merged_trainY\n",
        "testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X)),axis=1)\n",
        "testY=merged_testY\n"
      ],
      "metadata": {
        "id": "dMpCBET3XLft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adb25c0-2897-4be1-a638-97b7b13b5c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCpUiJTkC-th",
        "outputId": "67c947ca-5899-442e-bcb3-5e79690fb954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(653, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVC(kernel=\"rbf\", C=30, gamma=\"auto\",break_ties=False, random_state=3,decision_function_shape='ovr',probability=True)\n",
        "model.fit(trainX, trainY)"
      ],
      "metadata": {
        "id": "2tV9I6IX1gbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882a6ab0-ec5d-46f4-de67-683d924adba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=30, gamma='auto', probability=True, random_state=3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testX)"
      ],
      "metadata": {
        "id": "BD_7haxJ6ZqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=classification_report(testY, predictions, target_names=labels)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "FRWGRSx-6BIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35820e8d-c9b3-4a63-d24e-bfefde23cf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "    Gigakiek       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        13\n",
            "   macro avg       1.00      1.00      1.00        13\n",
            "weighted avg       1.00      1.00      1.00        13\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent dimension reduction modules"
      ],
      "metadata": {
        "id": "3Y30hQFkANbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "from random import random"
      ],
      "metadata": {
        "id": "naG8VNqLXSDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(gradient, start ,minx , maxx, learn_rate, stop_time, n_iter):\n",
        "    vector = start\n",
        "    start_time = timer()\n",
        "    for _ in range(n_iter):\n",
        "        diff = -learn_rate * gradient(vector)\n",
        "        vector += diff\n",
        "        if vector < minx:\n",
        "          vector=minx\n",
        "        elif vector > maxx:\n",
        "          vector=maxx\n",
        "        if ((timer()-start_time)*1000>stop_time):\n",
        "          break\n",
        "    return vector"
      ],
      "metadata": {
        "id": "M9AfFs1yzOSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_fit_transform(data,pol_deg, learn_rate=0.1, stop_time = 30, n_iter=200):\n",
        "  # pca_int = PCA(svd_solver=\"randomized\",n_components=pol_deg+1, whiten=True)\n",
        "  # data=pca_int.fit_transform(data)\n",
        "  vectors=[]\n",
        "  for vec in data:\n",
        "    polynomials=[]\n",
        "    for i in range(math.ceil(len(vec)/(pol_deg+1))):\n",
        "      points=vec[i*(pol_deg+1):i*(pol_deg+1)+pol_deg]\n",
        "      xrange=np.arange(len(points))\n",
        "      lagrange_interpolation = scipy.interpolate.lagrange(xrange, points)\n",
        "      grad = np.polyder(lagrange_interpolation, m=1)\n",
        "      # print(str(min(xrange))+\"   \"+str(max(xrange)))\n",
        "      x=gradient_descent(gradient=grad, start=random()*pol_deg,minx=min(xrange),maxx=max(xrange), learn_rate=learn_rate,n_iter=n_iter,stop_time=stop_time)\n",
        "      polynomials.append(x)\n",
        "      polynomials.append(lagrange_interpolation(x))\n",
        "    vectors.append(np.array(polynomials))\n",
        "      # Polynomial(lagrange_interpolation.coef[::-1]).coef\n",
        "  return np.array(vectors)"
      ],
      "metadata": {
        "id": "dqKGnr4bpF4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline1"
      ],
      "metadata": {
        "id": "M-nEh_Zd9K52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gradient_descent_transform(data,pol_deg,pca_int,learn_rate,n_iter,stop_time):\n",
        "#   data=pca_int.transform(data)\n",
        "#   polynomials=[]\n",
        "#   for vec in data:\n",
        "#     xrange=np.arange(len(vec))\n",
        "#     lagrange_interpolation = scipy.interpolate.lagrange(xrange, vec)\n",
        "#     grad = np.polyder(lagrange_interpolation, m=1)\n",
        "#     x=gradient_descent(gradient=grad, start=random()*pol_deg,minx=min(xrange),maxx=max(xrange), learn_rate=learn_rate,n_iter=n_iter,stop_time=stop_time)\n",
        "#     polynomials.append([x,lagrange_interpolation(x)])\n",
        "#     # Polynomial(lagrange_interpolation.coef[::-1]).coef\n",
        "#   return np.array(polynomials)"
      ],
      "metadata": {
        "id": "ceuiVUk8FYbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction1():\n",
        "  root=\"/content/drive/MyDrive/sem3_biom_bazdana\"\n",
        "  global labels\n",
        "  labels = []\n",
        "  image_matrix = []\n",
        "  image_labels = []\n",
        "  wrist_matrix=[]\n",
        "  signature_features_matrix=[]\n",
        "  signature_labels=[]\n",
        "  wrist_labels=[]\n",
        "  for user in os.listdir(f'{root}'):\n",
        "      labels.append(user)\n",
        "      for filename in os.listdir(f'{root}/{user}/faces'):\n",
        "\n",
        "          img = cv2.imread(f'{root}/{user}/faces/{filename}')\n",
        "          # print(img.shape)\n",
        "          img = cv2.resize(img, (120, 120))\n",
        "          # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "          # img = img.flatten()\n",
        "          image_matrix.append(np.array(img))\n",
        "          image_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/signatures'):\n",
        "          signature=read_signature(f'{root}/{user}/signatures/{filename}')\n",
        "          # print(img.shape)\n",
        "          stats = get_stats(signature)\n",
        "          # if stats[1]>0 and stats[0]<1:\n",
        "          signature_features_matrix.append(stats)\n",
        "          signature_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/wrist_gyroscope'):\n",
        "          # print(f'{root}/{user}/wrist_gyroscope/{filename}')\n",
        "          wrist_matrix.append(read_wrist_csv2(f'{root}/{user}/wrist_gyroscope/{filename}'))\n",
        "          # print(img.shape)\n",
        "          wrist_labels.append(labels.index(user))\n",
        "         \n",
        "  IMG_SIZE = (120, 120)\n",
        "  IMG_SHAPE = IMG_SIZE + (3,)\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "  preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  inputs = tf.keras.Input(shape=(120,120, 3))\n",
        "  x = preprocess_input(inputs)\n",
        "  x = base_model(x, training=False)\n",
        "  outputs = global_average_layer(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  image_matrix=model.predict(np.array(image_matrix))\n",
        "  global seed\n",
        "  train_sig_X, train_sig_Y, test_sig_X, test_sig_Y = split_mono_set(signature_features_matrix, signature_labels,seed)\n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y = split_mono_set(image_matrix, image_labels,seed)\n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y = split_mono_set(wrist_matrix, wrist_labels,seed)\n",
        "  return train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, train_img_X, train_img_Y, test_img_X, test_img_Y, train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y"
      ],
      "metadata": {
        "id": "jzcX_V2-9Dw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformations_concatenation1(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                     train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                     train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y):\n",
        "  pca = PCA(svd_solver=\"randomized\",n_components=comp_img, whiten=True)\n",
        "  # scaler_img = StandardScaler()\n",
        "  train_img_X = pca.fit_transform(np.array(train_img_X))\n",
        "  # train_img_X = scaler_img.fit_transform(np.array(train_img_X))\n",
        "  # print(train_img_X[0])\n",
        "\n",
        "  scaler_sig = StandardScaler() #\n",
        "  train_sig_X=scaler_sig.fit_transform(train_sig_X)\n",
        "  pca_sig = PCA(svd_solver=\"randomized\", n_components=comp_sig, whiten=True)\n",
        "  train_sig_X=pca_sig.fit_transform(train_sig_X)\n",
        "  # train_sig_X=np.fliplr(train_sig_X)\n",
        "\n",
        "  scaler_wrist= StandardScaler()\n",
        "  train_wrist_X=scaler_wrist.fit_transform(train_wrist_X)\n",
        "  pca_wrist = PCA(svd_solver=\"randomized\", n_components=comp_wrist, whiten=True)\n",
        "  train_wrist_X=pca_wrist.fit_transform(train_wrist_X)\n",
        "  # train_wrist_X=np.fliplr(train_wrist_X)\n",
        "\n",
        "  test_img_X=pca.transform(np.array(test_img_X))\n",
        "  # test_img_X = scaler_img.transform(np.array(train_img_X))\n",
        "\n",
        "  test_sig_X = scaler_sig.transform(np.array(test_sig_X))\n",
        "  test_sig_X = pca_sig.transform(test_sig_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  test_wrist_X = scaler_wrist.transform(np.array(test_wrist_X))\n",
        "  test_wrist_X = pca_wrist.transform(test_wrist_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  merged_trainX, merged_trainY = merge_sets_cartesian(train_img_X, train_sig_X, train_wrist_X, train_img_Y,\n",
        "                                                      train_sig_Y,train_wrist_Y)\n",
        "  train_img_X, train_sig_X, train_wrist_X = split_sets_back(merged_trainX)\n",
        "\n",
        "  merged_testX, merged_testY = merge_sets_simple(test_img_X, test_sig_X, test_wrist_X, test_img_Y,\n",
        "                                                      test_sig_Y,test_wrist_Y)\n",
        "  test_img_X, test_sig_X, test_wrist_X = split_sets_back(merged_testX)\n",
        "\n",
        "  trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X),np.array(train_wrist_X)),axis=1)\n",
        "  trainY=merged_trainY\n",
        "  testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X),np.array(test_wrist_X)),axis=1)\n",
        "  testY=merged_testY\n",
        "  return trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "4FqDGPar_tjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_testing_report1(trainX, trainY, testX, testY):\n",
        "  class_model = SVC(kernel=\"rbf\", C=30, gamma=\"auto\",break_ties=False, random_state=3,decision_function_shape='ovr',probability=True)\n",
        "  class_model.fit(trainX, trainY)\n",
        "  predictions = class_model.predict(testX)\n",
        "  res=classification_report(testY, predictions, target_names=labels)\n",
        "  # print(res)\n",
        "  return class_model,f1_score(testY, predictions,average='micro')"
      ],
      "metadata": {
        "id": "q4Rbpm47COZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call pipeline1"
      ],
      "metadata": {
        "id": "gouWYoR9KGnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=20\n",
        "decision_threshold=0.3\n",
        "comp_img=20\n",
        "comp_sig=7\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "GfFGSjLPC_xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_arr=[]\n",
        "for i in range(10):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction1()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation1(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report1(trainX, trainY, testX, testY)\n",
        "  res_arr.append(res)\n",
        "print(np.mean(res_arr))\n",
        "print(np.std(res_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJmeoc0J-yMB",
        "outputId": "77902689-7ea1-450a-af3e-dfe55416b89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       0.67      1.00      0.80         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.90      0.89        11\n",
            "weighted avg       0.94      0.91      0.90        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       0.67      1.00      0.80         2\n",
            "       Lukas       1.00      0.67      0.80         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.93      0.92        11\n",
            "weighted avg       0.94      0.91      0.91        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        11\n",
            "   macro avg       1.00      1.00      1.00        11\n",
            "weighted avg       1.00      1.00      1.00        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      0.50      0.67         2\n",
            "      Szymon       0.67      1.00      0.80         2\n",
            "       Pawel       0.67      1.00      0.80         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.82        11\n",
            "   macro avg       0.87      0.80      0.79        11\n",
            "weighted avg       0.88      0.82      0.81        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       0.67      1.00      0.80         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.90      0.89        11\n",
            "weighted avg       0.94      0.91      0.90        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       0.50      1.00      0.67         2\n",
            "       Lukas       1.00      0.67      0.80         3\n",
            "       Kowal       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.82        11\n",
            "   macro avg       0.90      0.83      0.83        11\n",
            "weighted avg       0.91      0.82      0.82        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      0.50      0.67         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       0.75      1.00      0.86         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.95      0.90      0.90        11\n",
            "weighted avg       0.93      0.91      0.90        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       0.67      1.00      0.80         2\n",
            "       Lukas       1.00      0.67      0.80         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.93      0.93      0.92        11\n",
            "weighted avg       0.94      0.91      0.91        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        11\n",
            "   macro avg       1.00      1.00      1.00        11\n",
            "weighted avg       1.00      1.00      1.00        11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       1.00      1.00      1.00         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      1.00      1.00         2\n",
            "       Lukas       1.00      1.00      1.00         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        11\n",
            "   macro avg       1.00      1.00      1.00        11\n",
            "weighted avg       1.00      1.00      1.00        11\n",
            "\n",
            "0.9181818181818182\n",
            "0.06363636363636363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c3751d-a3f5-47a2-9ead-b0dbe521c86a",
        "id": "R5xQSoIAvvAQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b733400-d5fc-4771-d128-17d368aee44b",
        "id": "jF5cj8oTvvAV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022727272727272728"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline2"
      ],
      "metadata": {
        "id": "WW1z8HdU9GhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction2():\n",
        "  root=\"/content/drive/MyDrive/sem3_biom_bazdana\"\n",
        "  global labels\n",
        "  labels = []\n",
        "  image_matrix = []\n",
        "  image_labels = []\n",
        "  wrist_matrix=[]\n",
        "  signature_features_matrix=[]\n",
        "  signature_labels=[]\n",
        "  wrist_labels=[]\n",
        "  for user in os.listdir(f'{root}'):\n",
        "      labels.append(user)\n",
        "      for filename in os.listdir(f'{root}/{user}/faces'):\n",
        "\n",
        "          img = cv2.imread(f'{root}/{user}/faces/{filename}')\n",
        "          # print(img.shape)\n",
        "          img = cv2.resize(img, (120, 120))\n",
        "          # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "          # img = img.flatten()\n",
        "          image_matrix.append(np.array(img))\n",
        "          image_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/signatures'):\n",
        "          signature=read_signature(f'{root}/{user}/signatures/{filename}')\n",
        "          # print(img.shape)\n",
        "          stats = get_stats(signature)\n",
        "          if stats[1]>0 and stats[0]<1:\n",
        "              signature_features_matrix.append(stats)\n",
        "              signature_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/wrist_gyroscope'):\n",
        "          # print(f'{root}/{user}/wrist_gyroscope/{filename}')\n",
        "          wrist_matrix.append(read_wrist_csv2(f'{root}/{user}/wrist_gyroscope/{filename}'))\n",
        "          # print(img.shape)\n",
        "          wrist_labels.append(labels.index(user))\n",
        "         \n",
        "  IMG_SIZE = (120, 120)\n",
        "  IMG_SHAPE = IMG_SIZE + (3,)\n",
        "  base_model = tf.keras.applications.vgg16.VGG16(input_shape=IMG_SHAPE,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "  preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  inputs = tf.keras.Input(shape=(120,120, 3))\n",
        "  x = preprocess_input(inputs)\n",
        "  x = base_model(x, training=False)\n",
        "  outputs = global_average_layer(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  image_matrix=model.predict(np.array(image_matrix))\n",
        "  global seed\n",
        "  train_sig_X, train_sig_Y, test_sig_X, test_sig_Y = split_mono_set(signature_features_matrix, signature_labels,seed)\n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y = split_mono_set(image_matrix, image_labels,seed)\n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y = split_mono_set(wrist_matrix, wrist_labels,seed)\n",
        "  return train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, train_img_X, train_img_Y, test_img_X, test_img_Y, train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y"
      ],
      "metadata": {
        "id": "HFzwQXVDBzi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_testing_report2(trainX, trainY, testX, testY):\n",
        "  class_model = GaussianNB()\n",
        "  class_model.fit(trainX, trainY)\n",
        "  predictions = class_model.predict(testX)\n",
        "  res=classification_report(testY, predictions, target_names=labels)\n",
        "\n",
        "  # print(res)\n",
        "  # print(f1_score(testY, predictions,average='micro'))\n",
        "  return class_model,f1_score(testY, predictions,average='micro')"
      ],
      "metadata": {
        "id": "MXCfwp7gL2Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call pipeline 2"
      ],
      "metadata": {
        "id": "-5FcT6TDvds5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=20\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=24\n",
        "comp_sig=8\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "Z4gPYMjiJ9Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res_arr=[]\n",
        "for i in range(10):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation1(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report2(trainX, trainY, testX, testY)\n",
        "  res_arr.append(res)\n",
        "print(np.mean(res_arr))\n",
        "print(np.std(res_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ox1MMjMlzv",
        "outputId": "dfe289d1-1461-4c4b-d857-4ba581f7d5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9636363636363636\n",
            "0.06030226891555271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHmyAYmLk_NR",
        "outputId": "756c523a-257e-48c3-b213-50dd0fef8fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09090909090909094"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7fca80hix2j",
        "outputId": "31cb47bd-9908-479e-96d4-38b3493dcc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022727272727272728"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline3"
      ],
      "metadata": {
        "id": "qzx3kj_79Adp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=27\n",
        "decision_threshold=0.5\n",
        "# comp_discard=3\n",
        "comp_img=20\n",
        "comp_sig=8\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "w0N6WkuR4gUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction3():\n",
        "  root=\"/content/drive/MyDrive/sem3_biom_bazdana\"\n",
        "  global labels\n",
        "  labels = []\n",
        "  image_matrix = []\n",
        "  image_labels = []\n",
        "  wrist_matrix=[]\n",
        "  signature_features_matrix=[]\n",
        "  signature_labels=[]\n",
        "  wrist_labels=[]\n",
        "  for user in os.listdir(f'{root}'):\n",
        "      labels.append(user)\n",
        "      for filename in os.listdir(f'{root}/{user}/faces'):\n",
        "\n",
        "          img = cv2.imread(f'{root}/{user}/faces/{filename}')\n",
        "          # print(img.shape)\n",
        "          img = cv2.resize(img, (120, 120))\n",
        "          img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "          img = img.flatten()\n",
        "          image_matrix.append(np.array(img))\n",
        "          image_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/signatures'):\n",
        "          signature=read_signature(f'{root}/{user}/signatures/{filename}')\n",
        "          # print(img.shape)\n",
        "          stats = get_stats(signature)\n",
        "          if stats[1]>0 and stats[0]<1:\n",
        "              signature_features_matrix.append(stats)\n",
        "              signature_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/wrist_gyroscope'):\n",
        "          # print(f'{root}/{user}/wrist_gyroscope/{filename}')\n",
        "          wrist_matrix.append(read_wrist_csv2(f'{root}/{user}/wrist_gyroscope/{filename}'))\n",
        "          # print(img.shape)\n",
        "          wrist_labels.append(labels.index(user))\n",
        "         \n",
        "\n",
        "  train_sig_X, train_sig_Y, test_sig_X, test_sig_Y = split_mono_set(signature_features_matrix, signature_labels,seed)\n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y = split_mono_set(image_matrix, image_labels,seed)\n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y = split_mono_set(wrist_matrix, wrist_labels,seed)\n",
        "  return train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, train_img_X, train_img_Y, test_img_X, test_img_Y, train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y"
      ],
      "metadata": {
        "id": "GbWQFkRt4nSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_testing_report3(trainX, trainY, testX, testY):\n",
        "  class_model = KNeighborsClassifier(n_neighbors=3)\n",
        "  class_model.fit(trainX, trainY)\n",
        "  predictions = class_model.predict(testX)\n",
        "  res=classification_report(testY, predictions, target_names=labels)\n",
        "  print(res)\n",
        "  return class_model,f1_score(testY, predictions,average='micro')"
      ],
      "metadata": {
        "id": "ErImF42B47MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "res_arr=[]\n",
        "for i in range(10):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction3()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation1(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report3(trainX, trainY, testX, testY)\n",
        "  res_arr.append(res)\n",
        "print(np.mean(res_arr))\n",
        "print(np.std(res_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RFnAXHc8FwR",
        "outputId": "67930af4-a0df-4378-e2ad-f84e8a086fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Mateusz       0.50      1.00      0.67         2\n",
            "      Szymon       1.00      1.00      1.00         2\n",
            "       Pawel       1.00      0.50      0.67         2\n",
            "       Lukas       1.00      0.67      0.80         3\n",
            "       Kowal       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.82        11\n",
            "   macro avg       0.90      0.83      0.83        11\n",
            "weighted avg       0.91      0.82      0.82        11\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504d58d3-55c4-48a2-8f8e-3b24c720d97d",
        "id": "q1A6FbZv7Yc_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18181818181818177"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAR_test(class_model, testX, testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6dbc0bc-3945-4fd9-f610-bd6d8b922010",
        "id": "N5YisVF77YdC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.045454545454545456"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient descent tests\n"
      ],
      "metadata": {
        "id": "jqo34UX-8080"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformations_concatenation_gradient_descent(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                     train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                     train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y):\n",
        "  pca = PCA(svd_solver=\"randomized\",n_components=comp_img, whiten=True)\n",
        "  # scaler_img = StandardScaler()\n",
        "  train_img_X = pca.fit_transform(np.array(train_img_X))\n",
        "  # train_img_X = scaler_img.fit_transform(np.array(train_img_X))\n",
        "\n",
        "\n",
        "  scaler_sig = StandardScaler() #\n",
        "  train_sig_X=scaler_sig.fit_transform(train_sig_X)\n",
        "  pca_sig = PCA(svd_solver=\"randomized\", n_components=comp_sig, whiten=True)\n",
        "  train_sig_X=pca_sig.fit_transform(train_sig_X)\n",
        "  # train_sig_X=np.fliplr(train_sig_X)\n",
        "\n",
        "  scaler_wrist= StandardScaler()\n",
        "  train_wrist_X=scaler_wrist.fit_transform(train_wrist_X)\n",
        "  pca_wrist = PCA(svd_solver=\"randomized\", n_components=comp_wrist, whiten=True)\n",
        "  train_wrist_X=pca_wrist.fit_transform(train_wrist_X)\n",
        "  # train_wrist_X=np.fliplr(train_wrist_X)\n",
        "\n",
        "  test_img_X=pca.transform(np.array(test_img_X))\n",
        "  # test_img_X = scaler_img.transform(np.array(train_img_X))\n",
        "\n",
        "  test_sig_X = scaler_sig.transform(np.array(test_sig_X))\n",
        "  test_sig_X = pca_sig.transform(test_sig_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  test_wrist_X = scaler_wrist.transform(np.array(test_wrist_X))\n",
        "  test_wrist_X = pca_wrist.transform(test_wrist_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  merged_trainX, merged_trainY = merge_sets_cartesian(train_img_X, train_sig_X, train_wrist_X, train_img_Y,\n",
        "                                                      train_sig_Y,train_wrist_Y)\n",
        "  train_img_X, train_sig_X, train_wrist_X = split_sets_back(merged_trainX)\n",
        "\n",
        "  merged_testX, merged_testY = merge_sets_simple(test_img_X, test_sig_X, test_wrist_X, test_img_Y,\n",
        "                                                      test_sig_Y,test_wrist_Y)\n",
        "  test_img_X, test_sig_X, test_wrist_X = split_sets_back(merged_testX)\n",
        "\n",
        "  #####\n",
        "\n",
        "  train_img_X= gradient_descent_fit_transform(np.array(train_img_X),pol_deg,learn_rate,n_iter,stop_time)\n",
        "  test_img_X = gradient_descent_fit_transform(test_img_X,pol_deg,learn_rate,n_iter,stop_time)\n",
        "\n",
        "  train_sig_X = gradient_descent_fit_transform(np.array(train_sig_X),pol_deg,learn_rate,n_iter,stop_time)\n",
        "  test_sig_X = gradient_descent_fit_transform(test_sig_X,pol_deg,learn_rate,n_iter,stop_time)\n",
        "\n",
        "  train_wrist_X = gradient_descent_fit_transform(np.array(train_wrist_X),pol_deg,learn_rate,n_iter,stop_time)\n",
        "  test_wrist_X = gradient_descent_fit_transform(test_wrist_X,pol_deg,learn_rate,n_iter,stop_time)\n",
        "  #####\n",
        "\n",
        "  if(mode == 'all'):\n",
        "    trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X),np.array(train_wrist_X)),axis=1)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X),np.array(test_wrist_X)),axis=1)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'face':\n",
        "    trainX=np.array(train_img_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_img_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'sig':\n",
        "    trainX=np.array(train_sig_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_sig_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'wrist':\n",
        "    trainX=np.array(train_wrist_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_wrist_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'face_sig':\n",
        "    trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X)),axis=1)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X)),axis=1)\n",
        "    testY=merged_testY\n",
        "  \n",
        "  return trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "xCxagaJB2TPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=27\n",
        "decision_threshold=0.3\n",
        "\n",
        "comp_img=24\n",
        "comp_sig=9\n",
        "comp_wrist=6"
      ],
      "metadata": {
        "id": "YERgUOyB1SDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pol_deg=2\n",
        "learn_rate=0.05\n",
        "n_iter=100\n",
        "stop_time=20\n",
        "mode='all'"
      ],
      "metadata": {
        "id": "ozlv4Xit7k69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_arr=[]\n",
        "for i in range(10):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation_gradient_descent(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model, res= training_testing_report2(trainX, trainY, testX, testY)\n",
        "  res_arr.append(res)\n",
        "print(np.mean(res_arr))\n",
        "print(np.std(res_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF_ucfUP1NIq",
        "outputId": "ce4c82ab-328c-4abb-bf95-b28902f32c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8818181818181818\n",
            "0.07100226978096956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSE evaluation\n"
      ],
      "metadata": {
        "id": "Ny9Wmu015kJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(class_model,X, Y):\n",
        "  X=np.array(X)\n",
        "  Y=np.array(Y)\n",
        "  dim=X.ndim\n",
        "  if (dim==1):\n",
        "    X = np.array([X])\n",
        "    Y = np.array([Y]) \n",
        "  res = np.array(class_model.predict_proba(X))\n",
        "  res_matrix=[]\n",
        "  for i in range(len(Y)-1):\n",
        "    res_matrix.append(1-res[i,Y[i]])\n",
        "  \n",
        "  res_matrix = np.array(res_matrix)\n",
        "\n",
        "  # max_cond = np.array(Y) == res_matrix[0]\n",
        "  return np.mean(np.square(res_matrix))"
      ],
      "metadata": {
        "id": "jQ8MoYxH_85k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSE(class_model,X, Y):\n",
        "  X=np.array(X)\n",
        "  Y=np.array(Y)\n",
        "  dim=X.ndim\n",
        "  if (dim==1):\n",
        "    X = np.array([X])\n",
        "    Y = np.array([Y]) \n",
        "  res = np.array(class_model.predict_proba(X))\n",
        "  res_matrix=[]\n",
        "  for i in range(len(Y)-1):\n",
        "    res_matrix.append(1-res[i,Y[i]])\n",
        "  \n",
        "  res_matrix = np.array(res_matrix)\n",
        "\n",
        "  # max_cond = np.array(Y) == res_matrix[0]\n",
        "  return np.mean(np.square(res_matrix))**(1/2)"
      ],
      "metadata": {
        "id": "_Md4kgLOSuJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MAE(class_model,X, Y):\n",
        "  X=np.array(X)\n",
        "  Y=np.array(Y)\n",
        "  dim=X.ndim\n",
        "  if (dim==1):\n",
        "    X = np.array([X])\n",
        "    Y = np.array([Y]) \n",
        "  res = np.array(class_model.predict_proba(X))\n",
        "  res_matrix=[]\n",
        "  for i in range(len(Y)-1):\n",
        "    res_matrix.append(1-res[i,Y[i]])\n",
        "  \n",
        "  res_matrix = np.array(res_matrix)\n",
        "\n",
        "  # max_cond = np.array(Y) == res_matrix[0]\n",
        "  return np.mean(res_matrix)"
      ],
      "metadata": {
        "id": "uekPK5mwWH4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FRR_test(class_model, testX, testY):\n",
        "  res = decision_module(class_model, testX, testY)\n",
        "  stat = np.sum(res)/res.shape[0]\n",
        "  stat = 1-stat\n",
        "  return stat"
      ],
      "metadata": {
        "id": "_TiVJpODCgeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FAR_test(class_model, testX, testY):\n",
        "  # add=np.random.random_integers(1, np.max(np.array(testY)), size = np.array(testY).shape[0])\n",
        "  # testY = (testY+ add) % (np.max(np.array(testY))+1)\n",
        "  testX_f=[]\n",
        "  testY_f=[]\n",
        "  for i in range(len(testY)):\n",
        "    for j in range(np.max(np.array(testY))):\n",
        "      testX_f.append(testX[i])\n",
        "      testY_f.append((testY[i]+j+1) % (np.max(np.array(testY))+1))\n",
        "  # print(testY_f)\n",
        "  # print(testY)\n",
        "  testX_f=np.array(testX_f)\n",
        "  testY_f=np.array(testY_f)\n",
        "  res = decision_module(class_model, testX_f, testY_f)\n",
        "  stat = np.sum(res)/res.shape[0]\n",
        "  return stat"
      ],
      "metadata": {
        "id": "ed0YPgpenk5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_results_print():\n",
        "  print(f'liczba rekordów w zbiorze treningowym: {len(trainX)}')\n",
        "  print(f'liczba rekordów w zbiorze testowym: {len(testX)}')\n",
        "  print('')\n",
        "  print(f'średni f1-score dla {test_count} podziałów: {np.mean(f1_arr)}')\n",
        "  print(f'odchylenie standardowe f1-score dla {test_count} podziałów: {np.std(f1_arr)}')\n",
        "  print('')\n",
        "  print(f'średni MSE dla {test_count} podziałów: {np.mean(MSE_arr)}')\n",
        "  print(f'odchylenie standardowe MSE dla {test_count} podziałów: {np.std(MSE_arr)}')\n",
        "  print('')\n",
        "  print(f'średni RMSE dla {test_count} podziałów: {np.mean(RMSE_arr)}')\n",
        "  print(f'odchylenie standardowe RMSE dla {test_count} podziałów: {np.std(RMSE_arr)}')\n",
        "  print('')\n",
        "  print(f'średni MAE dla {test_count} podziałów: {np.mean(MAE_arr)}')\n",
        "  print(f'odchylenie standardowe MAE dla {test_count} podziałów: {np.std(MAE_arr)}')"
      ],
      "metadata": {
        "id": "scrrp5lryr8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSE for multimodal system"
      ],
      "metadata": {
        "id": "03WQnj9Mn0Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=20\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=20\n",
        "comp_sig=7\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "UHXxm0xjFQ2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "f1_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation1(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report2(trainX, trainY, testX, testY)\n",
        "\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqbH97XEEywA",
        "outputId": "490db6af-8a36-47f2-eb80-b8fb49498a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y9W_hJNMTva",
        "outputId": "27df0faf-f2b9-4995-e2dc-1de2fb94dcf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 653\n",
            "liczba rekordów w zbiorze testowym: 13\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.9461538461538461\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.06007884366082041\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.04680927488525505\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.05450637473324295\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.15318029965711968\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.1527909378275102\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.052295757009930645\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.0578015537207285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSE for subsystems"
      ],
      "metadata": {
        "id": "4rS7czo1o7YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformations_concatenation2(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                     train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                     train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y):\n",
        "  pca = PCA(svd_solver=\"randomized\",n_components=comp_img, whiten=True)\n",
        "  # scaler_img = StandardScaler()\n",
        "  train_img_X = pca.fit_transform(np.array(train_img_X))\n",
        "  # train_img_X = scaler_img.fit_transform(np.array(train_img_X))\n",
        "\n",
        "\n",
        "  scaler_sig = StandardScaler() #\n",
        "  train_sig_X=scaler_sig.fit_transform(train_sig_X)\n",
        "  pca_sig = PCA(svd_solver=\"randomized\", n_components=comp_sig, whiten=True)\n",
        "  train_sig_X=pca_sig.fit_transform(train_sig_X)\n",
        "  # train_sig_X=np.fliplr(train_sig_X)\n",
        "\n",
        "  scaler_wrist= StandardScaler()\n",
        "  train_wrist_X=scaler_wrist.fit_transform(train_wrist_X)\n",
        "  pca_wrist = PCA(svd_solver=\"randomized\", n_components=comp_wrist, whiten=True)\n",
        "  train_wrist_X=pca_wrist.fit_transform(train_wrist_X)\n",
        "  # train_wrist_X=np.fliplr(train_wrist_X)\n",
        "\n",
        "  test_img_X=pca.transform(np.array(test_img_X))\n",
        "  # test_img_X = scaler_img.transform(np.array(train_img_X))\n",
        "\n",
        "  test_sig_X = scaler_sig.transform(np.array(test_sig_X))\n",
        "  test_sig_X = pca_sig.transform(test_sig_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  test_wrist_X = scaler_wrist.transform(np.array(test_wrist_X))\n",
        "  test_wrist_X = pca_wrist.transform(test_wrist_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "  merged_trainX, merged_trainY = merge_sets_simple(train_img_X, train_sig_X, train_wrist_X, train_img_Y,\n",
        "                                                      train_sig_Y,train_wrist_Y)\n",
        "  train_img_X, train_sig_X, train_wrist_X = split_sets_back(merged_trainX)\n",
        "\n",
        "  merged_testX, merged_testY = merge_sets_simple(test_img_X, test_sig_X, test_wrist_X, test_img_Y,\n",
        "                                                      test_sig_Y,test_wrist_Y)\n",
        "  test_img_X, test_sig_X, test_wrist_X = split_sets_back(merged_testX)\n",
        "\n",
        "  #####\n",
        "\n",
        "  if(mode == 'all'):\n",
        "    trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X),np.array(train_wrist_X)),axis=1)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X),np.array(test_wrist_X)),axis=1)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'face':\n",
        "    trainX=np.array(train_img_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_img_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'sig':\n",
        "    trainX=np.array(train_sig_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_sig_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'wrist':\n",
        "    trainX=np.array(train_wrist_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_wrist_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'face_sig':\n",
        "    trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X)),axis=1)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X)),axis=1)\n",
        "    testY=merged_testY\n",
        "\n",
        "    #####\n",
        "  return trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "1vQGftvxo6qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_results():\n",
        "  # seed=20\n",
        "  global seed\n",
        "  MSE_arr = []\n",
        "  RMSE_arr = []\n",
        "  MAE_arr = []\n",
        "  test_count=10\n",
        "  for i in range(test_count):\n",
        "    seed+=1\n",
        "    (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "    train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "    train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "    (trainX, trainY, testX, testY)=transformations_concatenation2(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                    train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                    train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "    class_model,res=training_testing_report2(trainX, trainY, testX, testY)\n",
        "    MSE_arr.append(MSE(class_model, testX, testY))\n",
        "    RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "    MAE_arr.append(MAE(class_model, testX, testY))\n",
        "  return MSE_arr, RMSE_arr, MAE_arr"
      ],
      "metadata": {
        "id": "fmb2Mz1htCya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For face\n"
      ],
      "metadata": {
        "id": "sZ552TA-pP5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=10\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=20\n",
        "comp_sig=7\n",
        "comp_wrist=4\n",
        "mode = 'face'"
      ],
      "metadata": {
        "id": "08TSSi7apLXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "f1_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation2(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report2(trainX, trainY, testX, testY)\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0219a6cf-a9d7-4dee-92c6-b7d1a9d9355c",
        "id": "_2Y7p2qcpLXe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in arccos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2tNV1uwsYGq",
        "outputId": "687ccaf7-f0f9-46e7-98c3-031ffdb99af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 50\n",
            "liczba rekordów w zbiorze testowym: 13\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.9461538461538461\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.0692307692307692\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.0555206584755077\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.06390745428481089\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.17529877583866482\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.157451572443635\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.06634714613445082\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.07126095393702972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For signature\n",
        "\n"
      ],
      "metadata": {
        "id": "djX_Ovhp2Tsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=20\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=20\n",
        "comp_sig=7\n",
        "comp_wrist=4\n",
        "mode = 'sig'"
      ],
      "metadata": {
        "id": "iXaB73Uc1V8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "f1_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation2(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report1(trainX, trainY, testX, testY)\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "id": "yCalYBVD1V8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98eb4bb-5da8-4782-8d86-dfc3327973bf",
        "id": "b3Zm6sbU1V8W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 50\n",
            "liczba rekordów w zbiorze testowym: 13\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.7923076923076924\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.10348941574672083\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.254617758494677\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.06318374047546532\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.5004522662797216\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.06453903989189572\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.4561532896520747\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.059893586884900166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For wrist\n",
        "\n"
      ],
      "metadata": {
        "id": "E6SjD4MY2gG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=10\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=20\n",
        "comp_sig=7\n",
        "comp_wrist=6\n",
        "mode = 'wrist'"
      ],
      "metadata": {
        "id": "9zXTnOh82gG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "f1_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)=feature_extraction2()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation2(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y, \n",
        "                                  train_wrist_X, train_wrist_Y, test_wrist_X, test_wrist_Y)\n",
        "  class_model,res=training_testing_report1(trainX, trainY, testX, testY)\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "id": "ERrGikak2gG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892980bc-fd58-4c2b-85d2-c00c4b08c0d1",
        "id": "b6V7SL3A2gG_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 50\n",
            "liczba rekordów w zbiorze testowym: 13\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.576923076923077\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.15858098560067924\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.4711543168298382\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.06694789041737687\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.6844987776738278\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.05114430753147296\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.6650982630051685\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.054806053692890086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No wrist"
      ],
      "metadata": {
        "id": "xtzNUrXarYTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_sets_cartesian_no_wrist(img_matrix, signature_matrix, image_labels,signature_labels):\n",
        "\n",
        "  # print(img_matrix[0])\n",
        "  merged_array2=[]\n",
        "  new_labels2=[]\n",
        "  for count, mer in enumerate(signature_matrix):\n",
        "    for count2, img in enumerate(img_matrix):\n",
        "      if(image_labels[count2] == signature_labels[count]):\n",
        "        merged_array2.append([img_matrix[count2], signature_matrix[count]])\n",
        "        new_labels2.append(image_labels[count2])\n",
        "  # print(np.array(merged_array).shape)\n",
        "  # print(merged_array[0])\n",
        "  return merged_array2,new_labels2"
      ],
      "metadata": {
        "id": "HjawDLPpthoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_sets_simple_no_wrist(img_matrix,signature_matrix,image_labels,signature_labels):\n",
        "  merged_array=[]\n",
        "  new_labels=[]\n",
        "  for count ,sig in enumerate(signature_matrix):\n",
        "    for count2, img in enumerate(img_matrix):\n",
        "      if(image_labels[count2]==signature_labels[count]):\n",
        "\n",
        "        merged_array.append([img_matrix[count2], signature_matrix[count]])\n",
        "        image_labels=np.delete(image_labels, count2, 0)\n",
        "        img_matrix=np.delete(img_matrix, count2, 0)\n",
        "        new_labels.append(signature_labels[count])\n",
        "        break\n",
        "  return merged_array,new_labels"
      ],
      "metadata": {
        "id": "y-nabcyIthoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sets_back_no_wrist(merged_matrix):\n",
        "    img_matrix=[]\n",
        "    sig_matrix=[]\n",
        "    wrist_matrix=[]\n",
        "    for  it in merged_matrix:\n",
        "        img_matrix.append(it[0])\n",
        "        sig_matrix.append(it[1])\n",
        "    return img_matrix,sig_matrix"
      ],
      "metadata": {
        "id": "bdCalPqlthoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction2_no_wrist():\n",
        "  root=\"/content/drive/MyDrive/BIOMETRIA_SEM3_baza_noc_gier/users\"\n",
        "  global labels\n",
        "  labels = []\n",
        "  image_matrix = []\n",
        "  image_labels = []\n",
        "  wrist_matrix=[]\n",
        "  signature_features_matrix=[]\n",
        "  signature_labels=[]\n",
        "  wrist_labels=[]\n",
        "  for user in os.listdir(f'{root}'):\n",
        "      labels.append(user)\n",
        "      for filename in os.listdir(f'{root}/{user}/faces'):\n",
        "\n",
        "          img = cv2.imread(f'{root}/{user}/faces/{filename}')\n",
        "          # print(img.shape)\n",
        "          img = cv2.resize(img, (120, 120))\n",
        "          # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "          # img = img.flatten()\n",
        "          image_matrix.append(np.array(img))\n",
        "          image_labels.append(labels.index(user))\n",
        "      for filename in os.listdir(f'{root}/{user}/signatures'):\n",
        "          signature=read_signature(f'{root}/{user}/signatures/{filename}')\n",
        "          # print(img.shape)\n",
        "          stats = get_stats(signature)\n",
        "          if stats[1]>0 and stats[0]<1:\n",
        "              signature_features_matrix.append(stats)\n",
        "              signature_labels.append(labels.index(user))\n",
        "      # for filename in os.listdir(f'{root}/{user}/wrist_gyroscope'):\n",
        "      #     # print(f'{root}/{user}/wrist_gyroscope/{filename}')\n",
        "      #     wrist_matrix.append(read_wrist_csv2(f'{root}/{user}/wrist_gyroscope/{filename}'))\n",
        "      #     # print(img.shape)\n",
        "      #     wrist_labels.append(labels.index(user))\n",
        "         \n",
        "  IMG_SIZE = (120, 120)\n",
        "  IMG_SHAPE = IMG_SIZE + (3,)\n",
        "  base_model = tf.keras.applications.vgg16.VGG16(input_shape=IMG_SHAPE,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "  preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  inputs = tf.keras.Input(shape=(120,120, 3))\n",
        "  x = preprocess_input(inputs)\n",
        "  x = base_model(x, training=False)\n",
        "  outputs = global_average_layer(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  image_matrix=model.predict(np.array(image_matrix))\n",
        "  global seed\n",
        "  train_sig_X, train_sig_Y, test_sig_X, test_sig_Y = split_mono_set(signature_features_matrix, signature_labels,seed)\n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y = split_mono_set(image_matrix, image_labels,seed)\n",
        "  \n",
        "  return train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, train_img_X, train_img_Y, test_img_X, test_img_Y"
      ],
      "metadata": {
        "id": "z5O1-ZpA2ZaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformations_concatenation_no_wrist(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                     train_img_X, train_img_Y, test_img_X, test_img_Y):\n",
        "  pca = PCA(svd_solver=\"randomized\",n_components=comp_img, whiten=True)\n",
        "  # scaler_img = StandardScaler()\n",
        "  train_img_X = pca.fit_transform(np.array(train_img_X))\n",
        "  # train_img_X = scaler_img.fit_transform(np.array(train_img_X))\n",
        "  # print(train_img_X[0])\n",
        "\n",
        "  scaler_sig = StandardScaler() #\n",
        "  train_sig_X=scaler_sig.fit_transform(train_sig_X)\n",
        "  pca_sig = PCA(svd_solver=\"randomized\", n_components=comp_sig, whiten=True)\n",
        "  train_sig_X=pca_sig.fit_transform(train_sig_X)\n",
        "  # train_sig_X=np.fliplr(train_sig_X)\n",
        "\n",
        "\n",
        "\n",
        "  test_img_X=pca.transform(np.array(test_img_X))\n",
        "  # test_img_X = scaler_img.transform(np.array(train_img_X))\n",
        "\n",
        "  test_sig_X = scaler_sig.transform(np.array(test_sig_X))\n",
        "  test_sig_X = pca_sig.transform(test_sig_X)\n",
        "  # test_sig_X = np.fliplr(test_sig_X)\n",
        "\n",
        "\n",
        "  merged_trainX, merged_trainY = merge_sets_cartesian_no_wrist(train_img_X, train_sig_X, train_img_Y,\n",
        "                                                      train_sig_Y)\n",
        "  train_img_X, train_sig_X = split_sets_back_no_wrist(merged_trainX)\n",
        "\n",
        "  merged_testX, merged_testY = merge_sets_simple_no_wrist(test_img_X, test_sig_X, test_img_Y,\n",
        "                                                      test_sig_Y)\n",
        "  test_img_X, test_sig_X = split_sets_back_no_wrist(merged_testX)\n",
        "\n",
        "  trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X)),axis=1)\n",
        "  trainY=merged_trainY\n",
        "  testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X)),axis=1)\n",
        "  testY=merged_testY\n",
        "\n",
        "\n",
        "    #####\n",
        "  if mode == 'face':\n",
        "    trainX=np.array(train_img_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_img_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'sig':\n",
        "    trainX=np.array(train_sig_X)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.array(test_sig_X)\n",
        "    testY=merged_testY\n",
        "  elif mode == 'face_sig':\n",
        "    trainX=np.concatenate((np.array(train_img_X),np.array(train_sig_X)),axis=1)\n",
        "    trainY=merged_trainY\n",
        "    testX=np.concatenate((np.array(test_img_X),np.array(test_sig_X)),axis=1)\n",
        "    testY=merged_testY\n",
        "\n",
        "    #####\n",
        "  return trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "uWMIkZn2srm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No wrist NB tests"
      ],
      "metadata": {
        "id": "oW-Ik65044_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=24\n",
        "decision_threshold=0.6\n",
        "mode='sig'\n",
        "# comp_discard=3\n",
        "comp_img=24\n",
        "comp_sig=12\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "rHNPl3QKvQQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_arr = []\n",
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y)=feature_extraction2_no_wrist()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation_no_wrist(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y)\n",
        "  class_model,res=training_testing_report2(trainX, trainY, testX, testY)\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "id": "q6yspf6bsbSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDbMoOzHX6aM",
        "outputId": "7107af17-ee74-48d5-9bb4-c7caf16665c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 2093\n",
            "liczba rekordów w zbiorze testowym: 44\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.7793164200140945\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.0611785651227693\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.19883401526767924\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.042705255691088094\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.44304588393554567\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.05044164941246656\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.23219146157298218\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.04143805792529576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No wrist SVC tests"
      ],
      "metadata": {
        "id": "BYn4tvS_49YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed=24\n",
        "decision_threshold=0.6\n",
        "# comp_discard=3\n",
        "comp_img=24\n",
        "comp_sig=12\n",
        "comp_wrist=4"
      ],
      "metadata": {
        "id": "dFOJQy9N5OWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_arr = []\n",
        "MSE_arr = []\n",
        "RMSE_arr = []\n",
        "MAE_arr = []\n",
        "test_count=10\n",
        "for i in range(test_count):\n",
        "  seed+=1\n",
        "  (train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "  train_img_X, train_img_Y, test_img_X, test_img_Y)=feature_extraction2_no_wrist()\n",
        "  (trainX, trainY, testX, testY)=transformations_concatenation_no_wrist(train_sig_X, train_sig_Y, test_sig_X, test_sig_Y, \n",
        "                                  train_img_X, train_img_Y, test_img_X, test_img_Y)\n",
        "  class_model,res=training_testing_report1(trainX, trainY, testX, testY)\n",
        "  f1_arr.append(res)\n",
        "  MSE_arr.append(MSE(class_model, testX, testY))\n",
        "  RMSE_arr.append(RMSE(class_model, testX, testY))\n",
        "  MAE_arr.append(MAE(class_model, testX, testY))"
      ],
      "metadata": {
        "id": "WJAP0S9U5OWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdeb375b-d1c6-4fdd-b8a9-6fbe2cfd6782",
        "id": "wxL7a2IP5OWs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "liczba rekordów w zbiorze treningowym: 2093\n",
            "liczba rekordów w zbiorze testowym: 44\n",
            "\n",
            "średni f1-score dla 10 podziałów: 0.9262634652169535\n",
            "odchylenie standardowe f1-score dla 10 podziałów: 0.022618279132417436\n",
            "\n",
            "średni MSE dla 10 podziałów: 0.18275861528460943\n",
            "odchylenie standardowe MSE dla 10 podziałów: 0.024954596164060077\n",
            "\n",
            "średni RMSE dla 10 podziałów: 0.4265290259708253\n",
            "odchylenie standardowe RMSE dla 10 podziałów: 0.02883756732091819\n",
            "\n",
            "średni MAE dla 10 podziałów: 0.3340543296544802\n",
            "odchylenie standardowe MAE dla 10 podziałów: 0.02842101789492631\n"
          ]
        }
      ]
    }
  ]
}